---
title: <center><h1> 2018R1 Regression in Practice (STAT5102)  Assignment 2</h1></center><br />
author: <center>Yiu Chung WONG 1155017920</center>
output:
  html_document:
    keep_md: yes
    code_folding:
  pdf_document: default
  word_document: default
--- 
<br />

```{r, echo = FALSE, results = 'hide'}
gc()
rm(list = ls())
```

```{r, echo = F, results = 'hide', message=FALSE, warning=FALSE}
library("knitr")
opts_chunk$set(echo = TRUE, cache=TRUE, message=FALSE, warning = FALSE)
options(width = 100)
library("ggplot2")
library("GGally")
library("dplyr")
```

```{r}
BGSall <- read.csv("BGSall.txt", header = TRUE, sep = " ")
BGSboys <- read.csv("BGSboys.txt", header = TRUE, sep = " ")
BGSgirls <- read.csv("BGSgirls.txt", header = TRUE, sep = " ")
maple <- read.table("maple.txt", header = TRUE, sep = "", stringsAsFactors = TRUE)
```

###1. 
####[Berkeley Guidance Study] The Berkeley Guidance Study enrolled children born in Berkeley, California, between January 1928 and June 1929, and then measured them periodically until age eighteen (Tuddenham and Snyder, 1954). The data we use is described in Table 1, and the data is given in the data files BGSgirls.txt for girls only, and BGSall.txt for boys and girls combined.

####a. For the girls only, draw the scatterplot matrix of all the age two variables, all the age nine variables and Soma. Write a summary of the information in this scatterplot matrix. Also obtain the matrix of sample correlations between the height variables
```{r message=FALSE, fig2, fig.height = 10, fig.width = 10, fig.align = "center"}
my.lm <- function(data, mapping, ...){
  p <- ggplot(data = data, mapping = mapping) + 
    geom_point(size = 2) + 
    geom_smooth(method = loess, fill = "red", color = "red", size = 1, ...) +
    geom_smooth(method = lm, fill = "blue", color = "blue", size = 1, ...)
  p
}


g <- ggpairs(
        BGSgirls, 
        columns = c(2:7, 12),
        upper = list(continuous = wrap("cor", size = 10, method = "spearman")),
        diag = list(continuous = wrap("barDiag", bins = sqrt(nrow(BGSgirls)))),
        lower = list(continuous = my.lm)
        )

g <- g + ggtitle("BGSgirls Age Two Correlation Matrix") + 
        theme(plot.title = element_text(lineheight = .8,
                                        face = "bold",
                                        size = 20), 
              axis.title.x = element_text(size = 10))

g
```
<br />

* `Soma` is the dependent variable to be used for the regression analysis. It appears to have moderate linear relations with 
several other variables such as `ST9` and `WT9`.

* Each predictors look fairly normal. Although OLS regression does not assume normality in the predictors, non-normal data **MAY** cause the relationship between them and the target variable to be non-linear. Looking at the bottom row and comparing loess curves (red) to the linear regression lines (blue) of all the graphs, the relationships between these predictors and `Soma` are relatively linear, so no linear transformations is required prior to modelling.
<br />

* In the same row, points are scattered around fairly evenly. This means variance are consistent across value of `Soma` and the predictors. Hence heteroskedasticity not likely to exist in the residual terms.

* Many predictors are highly correlated to each other. For example, `HT9` and `WT9` have correlation `r round(cor(BGSgirls$HT9, BGSgirls$WT9),3)`; `LG9` and `WT9` have correlation `r round(cor(BGSgirls$LG9, BGSgirls$WT9),3)`. Collinearity can also hide amongst the whole set of predictors instead of just two. Therefore, we are not safe from  multicollinearity simply because other predictors are of lower correlation. Other metrics such as Variance Inflation Factor is better at simultaneously evaluating one predictor against all others. One must be caution of multicollinearity in this dataset because they reduces the precision of the estimate coefficients (by weakening statistical power), as well as making coefficients sensitive (varies greatly) under the presence of other predictors in the model. 

```{r}
cor(BGSgirls[,c(3, 5, 9)])
```
<br />

####b. Fit the multiple linear regression model with mean function

$$
\begin{align*}
 \operatorname{E} (Soma \mid X ) = \beta_0 + \beta_1 HT2 + \beta_2 WT2 + \beta_3 HT9 + \beta_4 WT9 + \beta_5 ST9   \\
\end{align*}
$$

####based on BGSall.txt. Find *$\sigma^2$*, *$R^2$*, the overall analysis of variance table and overall F-test.  Compute the t-statistics to be used to test each of the *$\beta_{j}$* to be zero against two-sided alternatives. Explicitly state the hypotheses tested and the conclusions.

```{r}
BGSall_model = lm(data = BGSall, Soma ~ HT2 + WT2 + HT9 + WT9 + ST9)
```

* *$\sigma^2$* = `r sigma(BGSall_model)^2` 
* *$R^2$* = `r summary(BGSall_model)$r.squared`
<br />
```{r}
null_model = lm(data = BGSall, Soma ~ 1)
anova(null_model, BGSall_model)
```


* $H_{0}$: (HT2 == WT2 == HT9 == WT9 == ST9 == 0)
* $H_{1}$: At least a \beta_{j} =/= 0
* $F$ = `r anova(null_model, BGSall_model)$F[2]`
* $p$ = `r anova(null_model, BGSall_model)$"Pr(>F)"[2]`
* The p-value of the F-test is smaller then .05 critical value, we reject $H_{0}$: at least one $\beta_{j}$ =/= 0
<br />
```{r}
#t-statistics
summary(BGSall_model)$coefficients
```

####HT2
* $H_{0}$: $\beta_{1}$ == 0
* $H_{1}$: $\beta_{1}$ =/= 0
* $T$ = `r summary(BGSall_model)$coefficients[2, 3]`
* $p$ = `r summary(BGSall_model)$coefficients[2, 4]`
* The p-value of the T-test is greater then .05 critical value, we cannot reject $H_{0}$. $\beta_{1}$ == 0
<br />

####WT2
* $H_{0}$: $\beta_{2}$ == 0
* $H_{1}$: $\beta_{2}$ =/= 0
* $T$ = `r summary(BGSall_model)$coefficients[3, 3]`
* $p$ = `r summary(BGSall_model)$coefficients[3, 4]`
* The p-value of the T-test is smaller then .05 critical value, we reject $H_{0}$. $\beta_{2}$ =/= 0
<br />

####HT9
* $H_{0}$: $\beta_{3}$ == 0
* $H_{1}$: $\beta_{3}$ =/= 0
* $T$ = `r summary(BGSall_model)$coefficients[4, 3]`
* $p$ = `r summary(BGSall_model)$coefficients[4, 4]`
* The p-value of the T-test is greater then .05 critical value, we cannot reject $H_{0}$. $\beta_{3}$ == 0
<br />

####WT9
* $H_{0}$: $\beta_{4}$ == 0
* $H_{1}$: $\beta_{4}$ =/= 0
* $T$ = `r summary(BGSall_model)$coefficients[5, 3]`
* $p$ = `r summary(BGSall_model)$coefficients[5, 4]`
* The p-value of the T-test is smaller then .05 critical value, we reject $H_{0}$. $\beta_{4}$ =/= 0
<br />

####ST9
* $H_{0}$: $\beta_{5}$ == 0
* $H_{1}$: $\beta_{5}$ =/= 0
* $T$ = `r summary(BGSall_model)$coefficients[6, 3]`
* $p$ = `r summary(BGSall_model)$coefficients[6, 4]`
* The p-value of the T-test is smaller then .05 critical value, we reject $H_{0}$. $\beta_{5}$ =/= 0
<br />

###2. 
####[A Genetic Study] Seeds sampled from trees in the eastern US and Canada were planted in a genetic study. The time of leafing out of these seedlings can be related to the latitude and mean July temperature of the place of origin of the seed. The variables are $X_{1}$ = latitude, $X_{2}$ = July mean temperature,and $Y$ = weightedmean index of leafing out time. ($Y$ is a measure of the degree to which the leafing out process has occurred. A high value is indicative that the leafing out process is well advanced.) The data is below and in the file maple.txt on Blackboard.

####a. Find the regression of `LeafIndex` on `Latitude`. Is `Latitude` a useful predictor of `LeafIndex`?
```{r}
a = lm(data = maple, LeafIndex ~ Latitude)
```
* $H_{0}$: $\beta_{1}$ == 0
* $H_{1}$: $\beta_{1}$ =/= 0
* $T$ = `r summary(a)$coefficients[2, 3]`
* $p$ = `r summary(a)$coefficients[2, 4]`
* The p-value of the T-test is smaller then .05 critical value, we reject $H_{0}$. $\beta_{1}$ =/= 0
* Since this is a simple linear regression, the F-test should yield the same result. Hence we can say that the model is able to explain significantly more variation of `LeafIndex` than its mean.
<br />

####b. Repeat part (a) for the regression of `LeafIndex` on `JulyTemp`.
```{r}
b = lm(data = maple, LeafIndex ~ JulyTemp)
```

* $H_{0}$: $\beta_{1}$ == 0
* $H_{1}$: $\beta_{1}$ =/= 0
* $T$ = `r summary(b)$coefficients[2, 3]`
* $p$ = `r summary(b)$coefficients[2, 4]`
* The p-value of the T-test is smaller then .05 critical value, we reject $H_{0}$. $\beta_{1}$ =/= 0
* Since this is a simple linear regression, the F-test should yield the same result. Hence we can say that the model is able to explain significantly more variation of `LeafIndex` than its mean.

<br />

####c. Find the regression of `LeafIndex` on `Latitude` and `JulyTemp`. Compare the results of this analysis with your results from (a) and (b). How different are the slope coefficients in each case. What best explains the differences in their values?

```{r}
c = lm(data = maple, LeafIndex ~ Latitude + JulyTemp)
summary(c)$coefficients
```

* For the Intercept, `Latitude` , and `JulyTemp`, the point estimates and standard error are all different across the three models. 
* In the simple linear case, both models yield significant regression coefficients.
* In multiple regression, only `Latitude` has coefficients smaller than the .05 critical value; the coefficient of `JulyTemp` have p-value greater than the .05 critical value, meaning it is not significantly different from zero.
* The difference in significance of the regression coefficients can be best explained by collinearity. If we take a look at the correlation between `Latitude` and `JulyTemp`,

```{r}
with(maple, cor(Latitude, JulyTemp))
```

we can see that the correlation is `r with(maple, cor(Latitude, JulyTemp))`. This tells us that the two predictors are highly dependent on each other; they are close to linearly dependent. When two predictors are near linearly dependent, their standard error inflates. This reduces the precision of the estimate coefficients (by weakening statistical power), as well as making coefficients sensitive (varies greatly) under the presence of other predictors in the model.


* In our case, we do not have enough statistical power to prove `JulyTemp` is significantly different from zero.
<br />

####d. Find ANOVA tables for the model in part (a) (`LeafIndex` = $\beta_{0}$ + $\beta_{1}$`Latitude` + $\epsilon$) and the model in part (c) (`LeafIndex` = $\beta_{0}$ + $\beta_{1}$`Latitude` + $\beta_{2}$`JulyTemp` + $\epsilon$). What parts of the row of the ANOVA table corresponding to Latitude are the same and what parts are different? To what formal hypothesis test does the p-value in the Latitude row of each ANOVA table correspond? Why are the p-values different?

```{r}
anova(a)
anova(c)
```

* Degrees of freedom, sum of squared error, and mean squared error are the same between the two tables.
* The F-values and the p-values are different between the two tables.
* $H_{0}$: $\beta_{1}$ == 0
* $H_{1}$: $\beta_{1}$ =/= 0
* The F-value is the ratio between variation explained by that particular predictor v.s. variation not explained by any predictors. In the full model (with `JulyTemp`), a small portion of the variation is explained away by `JulyTemp`. Hence there are less unexplained variation in the full model than in the partial model (without `JulyTemp`). Because of this, the ratio between the variation explained by `Latitude` and unexplained variation is greater in the full model: MSE of `Latitude` in the full model is divided by a smaller number compared to that in the partial model. 
* A different F-value coupled with a slightly different degrees of freedom in the residuals yield a different p-value.