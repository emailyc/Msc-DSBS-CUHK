---
title: <center><h1>2019R2 High-Dimensional Data Analysis (STAT5103) Assignment 3</h1></center><br />
author: <center>Yiu Chung WONG 1155017920</center>
output:
  pdf_document: default
  html_document:
    keep_md: yes
  word_document: default
--- 
<br />
<br />


```{r, echo=FALSE}
pcaCharts <- function(x) {
    x.var <- x$sdev ^ 2
    x.pvar <- x.var/sum(x.var)
    print("proportions of variance:")
    print(x.pvar)
    
    par(mfrow=c(2,2))
    plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
    plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
    screeplot(x)
    screeplot(x,type="l")
    par(mfrow=c(1,1))
}

# mat : is a matrix of data
# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
```

# Principal Component Analysis (PCA) on uscrime Dataset

```{r, echo = F, results = 'hide', message=FALSE, warning=FALSE}
library("knitr")
opts_chunk$set(echo = TRUE, cache=TRUE, message=FALSE, warning = FALSE)
options(width = 100)
library("ggplot2")
library("GGally")
library("dplyr")
```

```{r}
# import data
teachers <- read.csv('hw3(2020).dat', header = FALSE, sep = '')
```

## a) 

### Correlation matrix

```{r}
# matrix of the p-value of the correlation
p.mat <- cor.mtest(teachers)

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot::corrplot(cor(teachers), method="color", col=col(200),
                  type="upper", order="hclust", 
                  addCoef.col = "black", # Add coefficient of correlation
                  tl.col="black", tl.srt=45, #Text label color and rotation
                  # Combine with significance
                  p.mat = p.mat, sig.level = 0.01, insig = "blank",
                  # hide correlation coefficient on the principal diagonal
                  diag=FALSE
                  )
```

* The colour indicates the strength of correlation: the deeper the blue, the more positive the correlation between two items. 
* Correlations that are not coloured are not statistically sigificant.
* Factor analysis assumes at least some items be correlated. From this graph we see that this assumption is valid.
* The items seem to assemble in two different groups. This suggest the possibility of two different latent factors behind the items.
<br />

### Bartlett's test of sphericity

```{r}
bartlett <- psych::cortest.bartlett(teachers)
bartlett
```

* $H_0$: All variables are independent
* p-value is `r bartlett$p.value`; $H_0$ is to be rejected
* Not all variables are independent

### KMO
```{r}
kmo <- psych::KMO(teachers)
kmo
```

* Overall MSA is `r kmo$MSA`, whcih is okay


### Subject/variable ratio
```{r}
n <- nrow(teachers)
p <- ncol(teachers)
ratio <- n/p
```
* The subject/variable ratio is `r ratio`. This number is low considering the suggested ratio should be around 10

### Scale of data

* The data is from a 10-item, 4-point Likert scale questionnaire. 
* Factor analysis assumes interval or ratio variables.
* If ordinal variable is to be used, there should be at least 5 categories. 

Eventhough the data show adaquit dependency, the number of category in items is just too low. This results in the strong non-normality in data. The subject to variable ratio is also not satisfacroty. Hence factor analysis on the provided data not recommended.
<br />
\newpage

## b)

### Principal Components Analysis
```{r}
 #Principle Component using non-centered, non-scaled datas
teachers_pca <- prcomp(teachers)
names(teachers_pca)
teachers_pca
```
```{r}
summary(teachers_pca)
```
```{r}
pcaCharts(teachers_pca)
```

* The scree plot presents the portion of variabce each principle component explains. 
* There is no obvious "elbow" indicating sigificant differences of variance explained. 

### Parallel Analysis
```{r}
psych::fa.parallel(x = teachers, fa = "pc", nfactors = p)
```

* Parallal analysis suggests the first two principle components exhibits eigenvalues higher than random data.
* This echos the two-group seperation presented by the correlation plot

### Principal Component Factor Analysis without rotation

```{r}
pcfa <- psych::fa(r = teachers, nfactors = 2, rotate = "none", fm = "pa")
pcfa_load <- pcfa$loadings[1:p,]
pcfa_com <- pcfa$communality
pcfa_psi <- pcfa$uniquenesses
pcfa_tbl <- cbind(pcfa_load, pcfa_com, pcfa_psi)
pcfa_tbl
pcfa$Vaccounted
```
<br />

# Maximum Likelihood Factor Analysis without rotation
```{r}
mlm <- psych::fa(teachers, nfactors = 2, rotate = "none", fm="ml")
mlm_load <- mlm$loadings[1:p,]
mlm_com <- mlm$communalities
mlm_psi <- mlm$uniquenesses 
mlm_tbl <- cbind(mlm_load, mlm_com, mlm_psi)
mlm_tbl
mlm$Vaccounted
```  
<br />
\newpage

Factor loadings can be interpreted like standardized regression coefficients, one could also say that the variable `r names(which.max(mlm_load[,1]))`` has a correlation of `r max(mlm_load[,1])` with Factor 1.

The precise value of each loading are not our main concern; we ar?re looking for groups of high values that hopefully make sense and lead to a descriptive factor. Without rotation, all 7 variables load on the first two axes and is currently impossible to see any patterns.

`Robbery` and `Auto_theft` have relatively high $\Psi$ value, this is bad because a high $\Psi$ indicates that particular variable is unique and does not load into any factor well. 

If we subtract the $\Psi$ value from 1, we get the column commonality. Commonality is the proportion of variance of the $i$th variable contributed by the m common factors. Looking at the commonality for the variable `r names(which.max(mlm_com))`, which has a value of `r which.max(mlm_com)`. This value can be interpreted as: `r scales::percent(which.max(mlm_com))` of the `r names(which.max(mlm_com))` variance was contributed by the two common factors. Since some of the $\Psi$ values are high, the two factors may not be explaining the overall variance so well. 

Sum of squared loadings tells us how much of all observed variance was explained by that factor. Here, the first factor is able to explain `r mlm$Vaccounted["SS loadings", 1]` units of variance. Some say a factor is worth keeping if the SS loading is greater than 1. This is the case for both factors factor.

The two factors explains roughly `r scales::percent(mlm$Vaccounted["Cumulative Var",2])` of the total variance. 

Since our factor loadings are difficult to interpret, perhaps we can get better results if we perform rotation on the loading.

# Maximum Likelihood Factor Analysis with varimax rotation
```{r}
mlmv <- psych::fa(teachers, nfactors = 2, rotate = "varimax", fm="ml")
mlmv_load <- mlmv$loadings[1:p,]
mlmv_com <- mlmv$communalities
mlmv_psi <- mlmv$uniquenesses 
mlmv_tbl <- cbind(mlmv_load, mlmv_com, mlmv_psi)
mlmv_tbl
mlmv$Vaccounted
```
<br />

After Varimax rotation, the factors are also a little more clear to interpret. `Murder`and `Assault`, are heavily loaded onto ML1. So it's clear that this is the Violence factor. The rest load heavily onto ML2, which maybe summarised as the 'Theft' factor. The variable `Rape` exhibits cross load; it is loaded onto both factors roughly 50/50. Perhaps Theft and Rape often occur at the same time, which does not sound surprising.

Both SS loadings remain greater than 1. Also, the SS loadings are more evenly divided between both factors than before rotation. The difference between the variance explained among the two factors also narrowed, but the sum remains the same. Therefore rotation is able to better separate the latent factors using our variables, but does not improve the relationship between variables and factors. This is also evident by looking at the $\Psi$ values, which are exactly the same as before rotation. 

# Principal Component Factor Analysis with varimax rotation
```{r}
pcfav <- psych::fa(r = teachers, nfactors = 2, rotate = "varimax", fm="pa")
pcfav_load <- pcfav$loadings[1:p,]
pcfav_com <- pcfav$communality
pcfav_psi <- pcfav$uniquenesses 
pcfav_tbl <- cbind(pcfav_load, pcfav_com, pcfav_psi)
pcfav_tbl
pcfav$Vaccounted
```

Principal Component Factor Analysis gives even clearer separation than Maximum Likelihood Factor Analysis. Variables in our 'Theft' factor 'Violence factor have more even loadings than before. 

The distance difference between the two methods can be seen in the $\Psi$ values. Principal Component Factor Analysis gives lower $\Psi$ values which sums up to `r sum(pcfa_psi)`; whereas $\Psi$ values from Maximum Likelihood Factor Analysis sums up to `r sum(sum(mlmv_psi))`.  By using Principal Component Factor Analysis, latent factors explains more variation of each of our variables. Both SS loadings and Proportion Variance are higher using Principal Component Factor Analysis. 

The fact that Principal Component Factor Analysis finds latent factors which explains more variation is because PCA is inherently a method for finding directions/rotations of maximum variance from data sets. 
<br />
\newpage

## d)