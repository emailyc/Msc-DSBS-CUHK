---
title: <center><h1> 2019R1 Discrete Data Analysis (STAT5107) Assignment 4</h1></center><br />
author: <center>Yiu Chung WONG 1155017920</center>
output:
  pdf_document: default
  html_document:
    keep_md: yes
  word_document: default
--- 
<br />
<br />

```{r, echo = FALSE,results = 'hide'}
gc()
rm(list = ls())
```


```{r, echo = F, results = 'hide', message=FALSE, warning=FALSE}
library("knitr")
opts_chunk$set(echo = TRUE, cache=TRUE, message=FALSE, warning = FALSE)
options(width = 100)
library("ggplot2")
library("dplyr")
```

```{r}
set.seed(5107);
```

#### 1.

$$
\frac{\pi(x+1) }{\pi(x)}
$$
is the relative risk between x+1 and x.

In a logit model
$$
\begin{aligned}
 \pi(x) = \frac{\exp(\alpha + \beta x)}{1+\exp(\alpha + \beta x)}
\end{aligned}
$$
assuming $0 \ne \Pr[Y=1] \ne 1$, the log-odd is defined as

$$
logit(\pi(x)) = \log\frac{\Pr[Y=1]}{\Pr[Y=0]} = \alpha + \beta x.
$$

if nudge the variable by a small value, we have
$$logit(\pi(x+\delta))$$

and the difference between the nudged log-odd and the original log-odd is defined as
$$
\begin{aligned}
  logit(\pi(x+\delta)) - logit(\pi(x)) &= (\alpha + \beta (x+\delta)) - (\alpha + \beta x )\\
  & =\beta \delta
\end{aligned}
$$

* $\beta$ is the change in log-odd when $x$ changes by $\delta$ unit.

let $\delta$ be $1$ and we have
$$
\begin{aligned}
  \exp(\beta) &= \exp(\beta \times 1) \\
  & = \exp( logit(\pi(x+1)) -  logit(\pi(x))) \\
  & = \frac{\frac{\pi(x+1)}{1 - \pi(x+1)}}{\frac{\pi(x)}{1 - \pi(x)}}
\end{aligned}
$$
$exp(\beta)$ is then the odds ratio for a one-unit increase in $x$.

* For a small success rate, the relative risk and odds ratio are close. 

<br />

#### 2a.
For

$$
\begin{aligned}
    logit(\pi(x)) &= \alpha + \beta \log(d)\\
\end{aligned}
$$

Then it can be rewritten as:

$$
\begin{aligned}
    log \frac{\pi(x)}{1 - \pi(x)} &= \alpha + \beta \log(d)\\
    \frac{\pi(x)}{1 - \pi(x)}     &= \exp(\alpha) \times \exp(\log(d^{\beta}))  \\
                                  &= \exp(\alpha) \times d^{\beta}  \\
\end{aligned}
$$
Then $d$ will be $1$ for the first draft pick

$$
\exp(\alpha) \times 1^{\beta} = \exp(\alpha)
$$


#### 2b.

On average, first drafts have probability of success
$$
\exp(\alpha)
$$

* For basketball, this is `exp(2.3)` = `r exp(2.3)`.
* For baseball, this is `exp(0.7)` = `r exp(0.7)`.

On average, for each unit increase in $d$, the probability of success is multiplied by $d^{\beta}$.

* For basketball, this is `d^{-1.1}`. This mean the probability of success has an inverse relationship with d.
* For baseball, this is `d^{-0.6}`. Probability of success also falls as $d$ increase but at a slower rate.

<br />

#### 3.
```{r}
alpha <- -3.7771;
beta <- 0.1449;
prob <- function(li)
{
  1/(1 + exp(-(alpha + li*beta)));
}
```
<br />

#### 3a.

```{r}
li <- 8;
prob(li);
```
<br />

#### 3b. 

```{r}
-alpha/beta;
```
<br />

#### 3c. 

```{r}
li_0.009 <- 8;
pi_0.009 <- prob(li_0.009);
rate_0.009 <- beta*(pi_0.009 * (1 - pi_0.009));

li_0.036 <- 26;
pi_0.036 <- prob(li_0.036);
rate_0.036 <- beta*(pi_0.036 * (1 - pi_0.036));

kable(data.frame(rate_0.009, rate_0.036))
```
<br />

#### 3d. 
```{r}
kable(data.frame(prob(14), prob(28)));
```
<br />

#### 3e.
```{r}
exp(beta);
```
<br />

#### 3f.
```{r}
beta <- 0.1449;
se <- 0.0593;
exp( qnorm(c(.025, .975), beta, se))
```
* Odds ratios are also not normally distributed. As a result, we must take the natural log of the odds ratio and first compute the confidence limits on a logarithmic scale.
<br />

\newpage

### 3g. 

$H_0$: $\beta$ is not sigificantly different to zero.
$H_1$: $\beta$ is sigificantly different to zero.
```{r}
H_0 <- 0;
z_wald <- (beta - H_0) / se;
cutoff <- qnorm(.975, 0, 1);
```
* For a large sample, the Wald statastic $z = \hat{\beta} / se(\hat{\beta})$ has a standard normal distribution when $\beta = 0$.
* The wald statistic is at `r z_wald` which is greater than the cutoff at `r cutoff`. 
* There is enough evidence to reject $H_0$.
<br />


### 3h.

* $H_0$: All $\beta$s are zero.
* $H_1$: At least a $\beta$s is not zero.
```{r}
intercept_only <- 34.372;
intercept_with_covariates <- 26.073;
deviance_diff <- intercept_only - intercept_with_covariates;
df <- 1;
deviance_diff > qchisq(.95, df);
```
* The test statistic is greater than the cutoff. There is enough evidence to reject $H_0$; we conclude at least a $\beta$s is not zero.


### 4a.

```{r}
intercept <- -3.5961;
def_beta <- -.8678;
vic_beta <- 2.4044;

death_prob <- function(def_race = 0, vic_race = 0)
{
  prob <- 1 / (1 + exp(-(intercept + def_beta * def_race + vic_beta * vic_race)));
  return(prob);
}
```

* Holding all other predictors constant, on average, the odds of death penalty for white over black defendant is `r exp(def_beta)`; white defendants are `r 100* (exp(def_beta))`% as likely to face death penalty.
* Holding all other predictors constant, on average, the odds of death penalty for white victim over black victim s `r exp(vic_beta)`, cases of white victim are `r 100* (exp(vic_beta))`% as likely to face death penalty.

```{r}
outer(X = 0:1, Y = 0:1, FUN = death_prob);
```
* Combination with highest probability of death penalty is: defendant = 0, victim = 1.
<br />

### 4b. 
```{r}
vic_lower <- 1.3068;
vic_upper <- 3.7175;
```
* Conditioned on defendantâ€™s race, the confidence interval for the effect on the odds from black to white victim equals ($e^{vic \ lower}$, $e^{vic \ upper}$) = (`r exp(vic_lower)`, `r exp(vic_upper)`). We infer that cases with white victims have at least has at least `r exp(vic_lower) * 100 - 100`% increase and at most `r round(exp(vic_upper))` fold in the odds that a given defendant faces death penalty.
<br />

### 4ci.
* $H_0$: $\beta$ for defendant is zero.
* $H_1$: $\beta$ for defendant is not zero.
```{r}
def_se <- 0.3671;
def_z <- def_beta/def_se;
normal_cutoff <- qnorm(c(.025, .975));
def_z < normal_cutoff[1];
```
* At 0.5 $\alpha$ level, the Wald test statistic is lower than the left side cutoff; there is enough evidence to reject $H_0$ and we conclude $\beta$ for defendant is not zero.
<br />

### 4cii.
* $H_0$: $\beta$ for defendant is zero.
* $H_1$: $\beta$ for defendant is not zero.
```{r}
chi_cutoff <- qnorm(.95, 1);
def_chi <- 5.59;
def_chi > chi_cutoff;
```
* With degrees of freedom of 1, the Chi-Square cutoff is at `r chi_cutoff`, which is lower than the Chi-Square statistic of the $\beta$ of the defendant variable. 
* There is enough evidence to reject $H_0$ and we conclude $\beta$ for defendant is not zero.